\documentclass{amsart}

\usepackage{amssymb}
\usepackage[margin=3cm]{geometry}
\usepackage{epigraph}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{tikz}

\begin{document}
\pagecolor{white}

\title{Red Queen's Sync Protocol for Ethereum}
\author{Andrew Ashikhmin \& Alexey Akhunov}
\date{April 2019}

\begin{abstract}
    As of early 2019, it takes new Ethereum full nodes a few hours
    to synchronise the blockchain state using the eth/63 protocol \cite{wire_protocol}.
    We propose a new protocol and an algorithm for Ethereum snapshot sync
    that is faster and more robust in respect of the growing state size.
    The new protocol is also tailored towards the needs of light clients
    and allows data storage formats other than the canonical Merkle Patricia trie.
    Performance results from a model implementation are encouraging.
\end{abstract}

\maketitle

\epigraph{
    "A slow sort of country!" said the Queen.
    "Now, here, you see, it takes all the running you can do, to keep in the same place.
    If you want to get somewhere else, you must run at least twice as fast as that!"
    }{Lewis Carroll, Through the Looking-Glass and What Alice Found There}

\section{Introduction}

It currently takes new Ethereum full nodes a few hours
to synchronise the blockchain state using the eth/63 protocol \cite{wire_protocol}.
Moreover, the growing state size might potentially result in complete sync failure,
as described in \cite{akhunov_1x_workshop_part1}.

As part of the Ethereum 1x effort, we propose a new sync protocol and algorithm,
which we call Red Queen's.
In our sync algorithm seeders reply with data as of their most recent block.
That results in an inconsistent trie on the leecher initially ("phase 1"),
which is patched later on ("phase 2").
The idea is similar to that of Leaf Sync (\cite{leaf_sync}).
Other sources of inspiration include BitTorrent, Parity's Warp Sync \cite{warp_sync}, and Firehose Sync \cite{firehose}.

Further, we strive to make the protocol work for light clients like Mustekala---see also \cite{light_client_protocol}.

N.B. In this document, we only discuss snapshot synchronisation rather than synchronisation from the Genesis block.

\section{Notation}

We mostly follow the conventions and notations of the Yellow Paper (\cite{yellow_paper}),
for instance, $\mathbb{Y}$ denotes the set of nibble sequences.
We use the letter $\pi$ for prefixes of state or storage trie keys $\mathbf{k} \in \mathbb{B}_{32}$,
\begin{equation}
    \pi \in \mathbb{Y} \; \land \; ||\pi|| \leq 64
\end{equation}
A key matches a prefix if and only if all their first nibbles are the same,
\begin{equation}
    \texttt{MATCH}(\mathbf{k}, \pi) \equiv \forall_{i < ||\pi||}: \mathbf{k}'[i] = \pi[i]
\end{equation}
($\mathbf{k}'$ is a sequence of nibbles, while $\mathbf{k}$ is a sequence of bytes.)

\section{Protocol Specification}

We propose the following new request/reply operative pairs\footnote{For
some extra information see Péter Szilágyi's
\href{https://ethereum-magicians.org/t/forming-a-ring-eth-v64-wire-protocol-ring/2857/10}{comment}
at ETH v64 Wire Protocol Ring.}:\\

\textbf{GetBytecode} (0x20)

[reqID: $\mathbb{N}$,
[codeHash$_0$: $\mathbb{B}_{32}$, codeHash$_1$: $\mathbb{B}_{32}$, ...]]
\medskip

Request EVM code of smart contracts.
Just like $\texttt{GetNodeData}$ from the current version (eth/63) of Ethereum Wire Protocol \cite{wire_protocol}, except:
\begin{enumerate}
\item includes a request ID;
\item will only return bytecode with the corresponding hash, not arbitrary node data.
\end{enumerate}
\bigskip

\textbf{Bytecode} (0x21)

[reqID: $\mathbb{N}$,
[code$_0$: $\mathbb{B}$, code$_1$: $\mathbb{B}$, ...]]
\medskip

Reply to $\texttt{GetBytecode}$.
Bytecode position in the response list must correspond to the position in the request list;
the empty list $\varnothing$ should be used for omitted bytecodes.\\

\textbf{GetStateNodes} (0x22)

[reqID: $\mathbb{N}$, stateRoot: $\mathbb{B}_{32}$,
[prefix$_0$: $\mathbb{Y}$, prefix$_1$: $\mathbb{Y}$, ...]]
\medskip

Request state trie nodes as of a specific state.
Note that this operative is similar to $\texttt{GetNodeData}$,
but it uses prefixes rather than hashes as node keys\footnote{In
a radix trie there is a trivial one-to-one correspondence between nodes and prefixes.
Since Ethereum employs a modified radix trie with extension nodes,
we define node's prefix as the shortest one such that
all leaves descending from the node match the prefix
and, conversely, all leaves that match the prefix descend from the node in question.}.
It will also only return nodes from the state trie,
not arbitrary node data.
The prefix encoding is described in Appendix C of \cite{yellow_paper}
(no additional flags are utilised).\\

\textbf{StateNodes} (0x23)

[reqID: $\mathbb{N}$,
[node$_0$: $\mathbb{B}$, node$_1$: $\mathbb{B}$, ...],
[availableState$_0$, ...]$_{opt}$]
\medskip

Reply to $\texttt{GetStateNodes}$.
The empty list $\varnothing$ returned instead of a node means that the peer does not have enough information about the node requested.
In that case the peer should return state roots for which requested nodes are available.\\

\textbf{GetAccounts} (0x24)

[reqID: $\mathbb{N}$, stateRoot: $\mathbb{B}_{32}$, [prefix$_0$: $\mathbb{Y}$, prefix$_1$: $\mathbb{Y}$, ...]]
\medskip

Request state trie leaves (i.e. accounts) as of a specific state.\\

\textbf{Accounts} (0x25)

[reqID: $\mathbb{N}$,

\quad [

\qquad [status$_0$, [[key$^0_{0}$: $\mathbb{B}_{32}$, val$^0_{0}$: $\mathbb{B}$], [key$^1_{0}$: $\mathbb{B}_{32}$, val$^1_{0}$: $\mathbb{B}$], ...]$_{opt}$],

\qquad [status$_1$, [[key$^0_{1}$: $\mathbb{B}_{32}$, val$^0_{1}$: $\mathbb{B}$], [key$^1_{1}$: $\mathbb{B}_{32}$, val$^1_{1}$: $\mathbb{B}$], ...]$_{opt}$],

\qquad ...

\quad ],

\quad [availableState$_0$, ...]$_{opt}$

]
\medskip

Reply to $\texttt{GetAccounts}$.
Returns accounts that match requested prefixes as key--value pairs\footnote{It
is feasible to return suffixes rather than full keys given that prefixes are known,
but we deem the performance gain to be insignificant.}.
TODO: status.\\

\textbf{GetStorageSizes} (0x26)

[reqID: $\mathbb{N}$, stateRoot: $\mathbb{B}_{32}$,
[addressHash$_0$: $\mathbb{B}_{32}$, addressHash$_1$: $\mathbb{B}_{32}$, ...]]
\medskip

Request storage trie sizes as of a specific state.\\

\textbf{StorageSizes} (0x27)

[reqID: $\mathbb{N}$,
[numLeaves$_0$: $\mathbb{N} | \varnothing$, numLeaves$_1$: $\mathbb{N} | \varnothing$, ...],
[availableState$_0$, ...]$_{opt}$]
\medskip

Reply to $\texttt{GetStorageSizes}$.
The peer may return the empty list $\varnothing$ instead of the number of leaves for accounts it does not have enough information about.
In that case the peer should return state roots for which requested data is available.\\

\textbf{GetStorageNodes} (0x28)

[reqID: $\mathbb{N}$, stateRoot: $\mathbb{B}_{32}$,

\quad [addressHash$^0$: $\mathbb{B}_{32}$, [prefix$^0_0$: $\mathbb{Y}$, prefix$^0_1$: $\mathbb{Y}$, ...]],

\quad [addressHash$^1$: $\mathbb{B}_{32}$, [prefix$^1_0$: $\mathbb{Y}$, prefix$^1_1$: $\mathbb{Y}$, ...]],

\quad ...

]
\medskip

Request storage trie nodes as of a specific state.\\

\textbf{StorageNodes} (0x29)

[reqID: $\mathbb{N}$,

\quad [

\qquad [node$^0_0$: $\mathbb{B}$, node$^0_1$: $\mathbb{B}$, ...],

\qquad [node$^1_0$: $\mathbb{B}$, node$^1_1$: $\mathbb{B}$, ...],

\qquad ...

\quad ],

\quad [availableState$_0$, ...]$_{opt}$

]
\medskip

Reply to $\texttt{GetStorageNodes}$.
The empty list $\varnothing$ returned instead of a node means that the peer does not have enough information about the node requested.\\

\textbf{GetStorageValues} (0x2a)

[reqID: $\mathbb{N}$, stateRoot: $\mathbb{B}_{32}$,

\quad [addressHash$^0$: $\mathbb{B}_{32}$, [prefix$^0_0$: $\mathbb{Y}$, prefix$^0_1$: $\mathbb{Y}$, ...]],

\quad [addressHash$^1$: $\mathbb{B}_{32}$, [prefix$^1_0$: $\mathbb{Y}$, prefix$^1_1$: $\mathbb{Y}$, ...]],

\quad ...

]
\medskip

Request storage subtrie leaves as of a specific state.\\

\textbf{StorageValues} (0x2b)

[reqID: $\mathbb{N}$,

\quad [

\qquad [

\quad \qquad [status$^0_0$, [[key$^0_{00}$: $\mathbb{B}_{32}$, val$^0_{00}$: $\mathbb{B}$], [key$^0_{01}$: $\mathbb{B}_{32}$, val$^0_{01}$: $\mathbb{B}$], ...]$_{opt}$],

\quad \qquad [status$^0_1$, [[key$^0_{10}$: $\mathbb{B}_{32}$, val$^0_{10}$: $\mathbb{B}$], [key$^0_{11}$: $\mathbb{B}_{32}$, val$^0_{11}$: $\mathbb{B}$], ...]$_{opt}$],

\quad \qquad ...

\qquad ],

\qquad [

\quad \qquad [status$^1_0$, [[key$^1_{00}$: $\mathbb{B}_{32}$, val$^1_{00}$: $\mathbb{B}$], [key$^1_{01}$: $\mathbb{B}_{32}$, val$^1_{01}$: $\mathbb{B}$], ...]$_{opt}$],

\quad \qquad [status$^1_1$, [[key$^1_{10}$: $\mathbb{B}_{32}$, val$^1_{10}$: $\mathbb{B}$], [key$^1_{11}$: $\mathbb{B}_{32}$, val$^1_{11}$: $\mathbb{B}$], ...]$_{opt}$],

\quad \qquad ...

\qquad ],

\qquad ...

\quad ],

\quad [availableState$_0$, ...]$_{opt}$

]
\medskip

Reply to $\texttt{GetStorageValues}$.
Returns storage values that match requested prefixes as key--value pairs.
TODO: status; how many is too many?
The peer may only return either all leaves of the subtrie or nothing.
Note that state trie replies do not inline storage tries, unlike Leaf Sync.

\section{Suggested Sync Algorithm}

Here we suggest a possible algorithm for full state and storage snapshot synchronisation using the protocol specified above;
light clients are out of scope.
We describe a modus operandi where the seeder replies with its most recent data,
and the leecher has to handle trie data coming from different blocks.
We suggest to perform synchronisation in two stages:
during phase~1 the leecher obtains leaf data (with the necessary proof nodes) as of any reasonable block height,
while during phase~2 it patches up the trie in order to catch up to the most recent block\footnote{The
Red Queen's race is a nice metaphor for phase 2.}.
The idea was proposed in \cite{leaf_sync}.

Let us focus on the state trie for the moment; we shall come back to storage sync later.
For phase 1 we suggest sending $\texttt{GetAccounts}$ requests with a single prefix per request, ditto for phase 2.
All requested prefixes are of size $d_1$ during phase 1 and of $d_2$ during phase 2, $d_2 \geq d_1$.
We elaborate on the values of $d_1$ and $d_2$ later.
The leecher gradually builds the first upper $d_2$ levels of the Merkle Patricia trie\footnote{$d_2$
is small enough so that we can reasonably assume that (almost) all nodes in question are branch nodes;
see \cite{akhunov_1x_workshop_part2}.}.
(The full trie can be constructed if so desired, but only the upper $d_2$ levels are necessary for our algorithm.)
Populated nodes are marked with the block number as of they are valid.
The algorithm preserves the following invariant: parent's block is always no older than child's block.

During phase 1 the leecher requests each possible prefix of size $d_1$ exactly once
(barring network failures and faulty peers).
When sending a request,
the leecher sets its $\texttt{blockAtLeast}$ to the block of the root of the current (partially populated) trie,
$\texttt{fromLevel}$ to the number of populated nodes down the path/prefix that are of the same block as the root.
Having received a reply, the leecher verifies its proof.
If the proof is valid, the leecher writes received leaves to the database and updates the nodes along the prefix/path.
By the end of phase 1, the leecher will have all accounts populated, albeit inconsistently.

\begin{figure}[h]
\begin{tikzpicture} [scale=1, auto=left]
\tikzstyle{selected edge} = [draw,line width=2pt,-,red!50]

  \node[circle,draw] (n)       at (4.2,4) {5};
  \node[circle,draw] (n7)     at (2.5,3) {3};
  \node[circle,draw] (n9)     at (6,3)    {5};
  \node[circle,draw] (n7a)   at (2.5,2) {3};
  \node[circle,draw] (n95)   at (6,2)    {5};
  \node[circle,draw] (n7a0) at (1,1)    {3};
  \node(n7a3) at (2,1)    {?};
  \node[circle,draw] (n7ab) at (3,1)    {2};
  \node[circle,draw] (n7af)  at (4,1)    {3};
  \node[circle,draw] (n958) at (5.2,1) {5};
  \node[circle,draw] (n959) at (6.8,1) {4};

  \draw[selected edge] (n) -- (n7);
  \draw (n) -- (n9);
  \draw[selected edge] (n7) -- (n7a);
  \draw (n9) -- (n95);
  \draw (n7a) -- (n7a0);
  \draw[selected edge] (n7a) -- (n7a3);
  \draw (n7a) -- (n7ab);
  \draw (n7a) -- (n7af);
  \draw (n95) -- (n958);
  \draw (n95) -- (n959);
  
  \node at (1.5, 1.5) {0};
  \node at (2.1, 1.5) {3};
  \node at (2.9, 1.5) {b};
  \node at (3.5, 1.5) {f};
  
  \node at (5.3, 1.5) {8};
  \node at (6.7, 1.5) {9};
  
  \node at (2.3, 2.5) {a};
  \node at (6.2, 2.5) {5};
  
  \node at (3.1, 3.6) {7};
  \node at (5.4, 3.6) {9};
\end{tikzpicture}
\setlength{\unitlength}{2cm}
\begin{picture}(1,1)
\put(0.2,0.9){\line(1,0){0.5}}
\put(0.2,1.1){\line(1,0){0.5}}
\put(0.8,1){\line(-1,1){0.2}}
\put(0.8,1){\line(-1,-1){0.2}}
\end{picture}
\begin{tikzpicture} [scale=1,auto=left]
  
  \node[circle,draw,fill=blue!20] (n)       at (4.2,4) {6};
  \node[circle,draw,fill=blue!20] (n7)     at (2.5,3) {6};
  \node[circle,draw] (n9)     at (6,3)    {5};
  \node[circle,draw,fill=blue!20] (n7a)   at (2.5,2) {6};
  \node[circle,draw] (n95)   at (6,2)    {5};
  \node[circle,draw] (n7a0) at (1,1)    {3};
  \node[circle,draw,fill=blue!20] (n7a3) at (2,1)    {6};
  \node[circle,draw] (n7ab) at (3,1)    {2};
  \node[circle,draw] (n7af)  at (4,1)    {3};
  \node[circle,draw] (n958) at (5.2,1) {5};
  \node[circle,draw] (n959) at (6.8,1) {4};

  \draw (n) -- (n7);
  \draw (n) -- (n9);
  \draw (n7) -- (n7a);
  \draw (n9) -- (n95);
  \draw (n7a) -- (n7a0);
  \draw (n7a) -- (n7a3);
  \draw (n7a) -- (n7ab);
  \draw (n7a) -- (n7af);
  \draw (n95) -- (n958);
  \draw (n95) -- (n959);
  
  \node at (1.5, 1.5) {0};
  \node at (2.1, 1.5) {3};
  \node at (2.9, 1.5) {b};
  \node at (3.5, 1.5) {f};
  
  \node at (5.3, 1.5) {8};
  \node at (6.7, 1.5) {9};
  
  \node at (2.3, 2.5) {a};
  \node at (6.2, 2.5) {5};
  
  \node at (3.1, 3.6) {7};
  \node at (5.4, 3.6) {9};
\end{tikzpicture}
\caption{Illustration of phase 1 sync. Nodes are labelled with block numbers, and edges are labelled with nibbles.}
\label{fig:phase1_example}
\end{figure}

Figure~\ref{fig:phase1_example} shows an example of a phase 1 step with $d_1 = 3$ and $d_2 = 4$.
Say the leecher is interested in prefix $<$7a3$>$.
The trie on the left represents leecher's state before sending a request.
Root's block is 5, so it sets $\texttt{blockAtLeast} = 5$.
The leecher sets $\texttt{fromLevel}$ to 1 since there is no need to re-send the root as part of the proof.
It cannot set $\texttt{fromLevel}$ higher as the other nodes along the path are older than the root and thus have to be refreshed.
Suppose that the seeder replies with data as of a newer block \#6.
Since the block has changed, the seeder ignores $\texttt{fromLevel}$ and sends full proof.
The leecher saves received leaves to its database and updates the nodes  ($<>$, $<$7$>$, $<$7a$>$, $<$7a3$>$).
The result is displayed on the right of Figure~\ref{fig:phase1_example}.

At the beginning of phase 2, the leecher updates the trie in order to figure out which subtries have to be refreshed.
For that, it uses the $\texttt{GetNodeData2}$ operative.
The leecher refreshes the trie level by level, starting from the root (level 0) and descending to the level $d_2 - 1$.
Nodes at the same level may be requested in batch.
Having refreshed nodes for a level, it knows which child nodes one level below have to be refreshed since the leecher can compare their hashes it currently holds with received fresh data.
Therefore, only the nodes that have actually changed need to be requested.
The leecher might have to restart the node refresh process from the root if new blocks are mined in between;
however, provided a certain network bandwidth, the process converges.
We analyse convergence conditions in the next section.

When all trie levels from the root down to the level $d_2 - 1$ are up to date,
the leecher knows which nibbles at that last level have changed since phase 1.
It refreshes the leaves corresponding to such nibbles using $\texttt{GetAccounts}$ requests. 
That concludes the algorithm for state trie synchronisation.

The algorithm for storage sync is similar for large storage tries.
Its parameters $d_1$ and $d_2$ are optimised based on the storage size as described in the next section.
Small storage tries can be obtained in bulk requesting the empty prefix (meaning the entire trie) for a number of them in one go.
$\texttt{GetStorageSizes}$ provides a means of finding out storage sizes.

\section{Performance Analysis}

In this analysis, we assume that all tries are well balanced.
We also assume that all top nodes up to a certain trie level $i$ are branch nodes, not leaf nor extension nodes.
This is a reasonable assumption if $i$ is not too big---see~\cite{akhunov_1x_workshop_part2}.
And we simplify the byte size function of the replies\footnote{Total
request size is much smaller than total reply size, so we ignore requests as well.},
ignoring overheads caused by auxiliary data such as $\texttt{reqID}$,
RLP encoding, and the network layer.
%TODO: internal notation consistency + cross-check against the Yellow Paper
Let us introduce some notation:

$n$ -- the average node size in bytes,
equal essentially to the size of a branch node as most nodes transferred will be branch nodes.
530 bytes is a good estimate.

$l$ -- the average leaf size in bytes, counting both key and value.
For the state trie it is the average account size plus the size of its hash key,
resulting in about 115 bytes.
TODO: storage trie.

$t$ -- total number of leaves in a trie.
For the state trie it is the number of accounts,
which is about $53 \cdot 10^6$ as of February 2019---see \cite{akhunov_1x_workshop_part2}.

$b$ -- the network bandwidth available to the leecher.

$\tau$ -- the block time, currently 15 seconds.

$\delta$ -- the average number of leaf changes per block for a trie.
For the state trie it is in the ballpark of 300.

$||R_n||$ -- the number of nodes in a reply $R$.

$||R_l||$ -- the number of leaves in a reply $R$.

We use the following simplified formula for the byte size of a reply $R$
\begin{equation}
    S(R) = ||R_n|| n + ||R_l|| l
\end{equation}

The overhead of the sync algorithm during phase 1, compared with Parity's warp sync, is in the proof nodes sent alongside the leaf data.
The overhead grows with $d_1$, so we want the trie depth to be as low as possible.
On the other hand, small $d_1$ implies a large number of leaves per reply, which can be brittle or inefficient.
Thus we set $d_1$ to the smallest value possible such that the replies are, on average, no larger than a certain size (say 32 KiB).
We denote that maximum size as $m$.
During phase~1 a $\texttt{Accounts}$ reply contains at most $d_1$ nodes
and its average number of leaves is $\frac{t}{16^{d_1}}$,
which gives us
\begin{equation}
    d_1 n + \frac{t}{16^{d_1}} l \leq m
\end{equation}
Fot the state trie the limit of 32 KiB yields $d_1 = 5$.

Let $C(d, \delta)$ be the maximum number of trie nodes from the upper $d$ levels of a trie that can change (on average) per block\footnote{To
be more precise mathematically, $C(d, \delta)$ is an upper bound on the expected value.}.
At each level at most $\delta$ nodes can change, subject to $\delta$ being smaller than the number of nodes at the level.
Thus
\begin{equation}
    C(d, \delta) = \sum_{i=0}^{d-1} \min(16^i, \delta)
\end{equation}
If $16^2 \leq \delta \leq 16^3$ and $d \geq 3$, then
\begin{equation}
    C(d, \delta) = C'(d, \delta) \overset{\underset{\mathrm{def}}{}}{=}
     \delta (d-3) + 273
\end{equation}

We now analyse the minimum bandwidth required for the algorithm to converge during phase 2.
At the very least, "to keep in the same place",
we need to sync all changes per 1 block no slower than the block time $\tau$.
As previously described, the algorithm updates $d_2$ upper levels of the trie.
So the upper bound on the number of nodes to be refreshed is $C(d_2, \delta)$.
The number of subtries that need to be refreshed is no more than $\delta$;
each subtrie has $\frac{t}{16^{d_2}}$ leaves on average.
Summing up, the total reply size per 1 block necessary not to lag behind is less than
\begin{equation}
    \texttt{RQS} \overset{\underset{\mathrm{def}}{}}{=}
    C(d_2, \delta) \, n + \delta \, \frac{t}{16^{d_2}} \, l
\end{equation}
(RQS stands for Red Queen's Size).
Though it is an upper bound, for our purposes $\texttt{RQS}$ is close enough to the actual value.
Differentiating, we find the value of $d_2$ that minimises $\texttt{RQS}$
\begin{equation}
    d_2^* = \frac{1}{\ln 16} \ln \left( \frac{tl \ln16}{n} \right)
\end{equation}
(Obviously, one has to round $d_2^*$ up or down.)
For the state trie the optimal $d_2^* = 6$ and the entailing $\texttt{RQS}$ is about 0.7 MiB.
Reiterating, the convergence condition for the state trie alone is
\begin{equation}
    b > \frac{\texttt{RQS}}{\tau}
\end{equation}
For the Ethereum main net as of February 2019 this critical minimum bandwidth is about 0.4 Mbit/s.
Table~\ref{tab:emulated_time} shows performance results of an emulation of the sync protocol for various state trie sizes.
The modelling code used is hosted at
\href{https://github.com/yperbasis/silkworm/blob/master/lab/sync_emulator.cpp}{https://github.com/yperbasis/silkworm}.
Network latency was ignored in the emulation.

\begin{table}[h]
\begin{tabular}{ r | c c c }
    Bandwidth & 10M & 50M & 100M \\
    \hline
      1 Mbit/s & 03:39 & 18:44 & 39:04 \\
     10 Mbit/s & 00:20 & 01:39 & 03:17 \\
    100 Mbit/s & 00:02 & 00:10 & 00:20 \\
\end{tabular}
\caption{Emulated times of state trie sync for 10M, 50M, and 100M dust accounts.}
\label{tab:emulated_time}
\end{table}

Convergence analysis for large storage tries (e.g. CryptoKitties) is similar to the state trie analysis above.

\section{Conclusion}

TODO: conclusion.

\bibliographystyle{plainnat}
\bibliography{biblio}

% TODO reference Firehose Sync, Light client protocol, etc.

\end{document}
